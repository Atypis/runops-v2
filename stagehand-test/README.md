# 🎭 Stagehand Test Environment

This directory contains a complete testing environment for validating **Stagehand AI agent execution** of SOPs generated by our giga-optimized prompt.

## 🎯 Purpose

Test the atomic granularity and execution reliability of SOPs generated by the **Stagehand-optimized prompt v0.91** compared to the original prompt.

## 🚀 Quick Start - Dynamic Authentication (Recommended)

The **dynamic authentication approach** is much more flexible and user-friendly:

```bash
# 1. Install dependencies
npm install

# 2. Test browser visibility first
npm run test-visibility

# 3. Run dynamic SOP execution (no upfront auth needed!)
npm run test-dynamic
```

### ✨ Benefits of Dynamic Auth:
- 🎭 **Browser opens immediately** - no waiting for credential setup
- 🔐 **AI requests credentials only when needed** - more natural workflow  
- 🤖 **Conversational approach** - AI "talks back" when it needs auth
- ⚡ **Faster startup** - no upfront configuration required
- 🔄 **Session caching** - credentials remembered during execution

## 🎯 Available Test Scripts

| Script | Purpose | Authentication |
|--------|---------|----------------|
| `npm run test-dynamic` | **Dynamic auth demo** (recommended) | On-demand prompts |
| `npm run test-visibility` | Enhanced browser visibility test | None |
| `npm run test-your-sop` | Your actual SOP workflow | Pre-configured |
| `npm run test-gmail` | Simple Gmail automation | Pre-configured |
| `npm run demo` | Visual demonstration | None |
| `npm run compare` | Multi-model comparison | Pre-configured |

## 🔧 Configuration

### LLM Configuration
Edit `stagehand.config.js` to configure your preferred AI model:

```javascript
export default {
  llmConfig: {
    modelName: "google/gemini-2.5-flash-preview-05-20", // or gpt-4o, claude-3-5-sonnet
    modelClientOptions: {
      apiKey: process.env.GEMINI_API_KEY, // or OPENAI_API_KEY, ANTHROPIC_API_KEY
    }
  }
};
```

### Environment Variables
Create `.env` file:
```bash
# Choose your preferred LLM
GEMINI_API_KEY=your_gemini_key_here
OPENAI_API_KEY=your_openai_key_here  
ANTHROPIC_API_KEY=your_anthropic_key_here

# Optional: Pre-configure credentials (not needed for dynamic auth)
GMAIL_EMAIL=your_email@gmail.com
GMAIL_PASSWORD=your_app_password
AIRTABLE_EMAIL=your_email@example.com
AIRTABLE_PASSWORD=your_password
```

## 🎭 Dynamic Authentication Flow

The dynamic approach works like this:

1. **Browser opens immediately** - no credential prompts upfront
2. **AI navigates to login screens** - Gmail, Airtable, etc.
3. **AI detects authentication needed** - analyzes page content
4. **System prompts for credentials** - only when actually needed
5. **Credentials cached for session** - no repeated prompts
6. **AI continues workflow** - seamless execution

### Example Dynamic Flow:
```
🚀 Browser opens → Gmail.com
🤖 AI: "I see a login screen"
🔐 System: "Gmail Authentication Required - Enter email:"
👤 User: enters email
🔐 System: "Enter password:" 
👤 User: enters password (masked)
✅ Credentials cached for session
🤖 AI: continues with inbox access
```

## 🎬 Execution Examples

### Dynamic Authentication (Recommended)
```bash
npm run test-dynamic
```
- Browser opens immediately
- AI requests auth when needed
- Natural conversational flow
- Session credential caching

### Enhanced Browser Visibility
```bash
npm run test-visibility  
```
- Tests browser window visibility
- macOS optimized settings
- DevTools integration
- Window positioning and focus

### Your Actual SOP
```bash
npm run test-your-sop
```
- Executes your recorded workflow
- 15 atomic steps from investor CRM
- Gmail → Airtable automation
- Pre-configured credentials

## 🔍 Browser Visibility Troubleshooting

If you can't see the browser window:

1. **Check all desktop spaces** - browser might open in different space
2. **Look for Chrome/Chromium** - window might be behind other apps
3. **Check Dock** - browser icon should appear when running
4. **Run visibility test**:
   ```bash
   npm run test-visibility
   ```
5. **macOS permissions** - ensure automation permissions are granted

### Enhanced Visibility Features:
- `--start-maximized` - large window size
- `--auto-open-devtools-for-tabs` - DevTools visible
- `--window-position=100,100` - specific positioning
- `--activate-on-open` - brings window to front
- `slowMo: 1000` - slower actions for observation

## 📊 Model Comparison

Test different AI models:
```bash
npm run compare
```

Supported models:
- **Gemini 2.5 Flash Preview** (recommended for speed)
- **GPT-4o** (excellent reasoning)
- **Claude 3.5 Sonnet** (great for complex tasks)
- **Ollama local models** (privacy-focused)

## 🔐 Security Features

### Dynamic Auth Security:
- **Memory-only storage** - credentials never written to disk
- **Session-scoped** - cleared when execution ends
- **Masked input** - passwords hidden during entry
- **No hardcoding** - credentials requested dynamically

### Pre-configured Auth Security:
- **Environment variables** - credentials in `.env` file
- **Interactive prompts** - secure password masking
- **Automatic cleanup** - memory cleared after execution

## 🎯 SOP Structure

Your SOPs should include these fields for optimal execution:

```json
{
  "id": "step_id",
  "type": "task|extract|decision", 
  "label": "Human readable description",
  "stagehand_instruction": "AI instruction with {{variable}} placeholders",
  "extract_instruction": "What to extract from page",
  "extract_schema": "Zod schema string",
  "error_recovery": ["fallback instruction 1", "fallback instruction 2"],
  "confidence_level": "high|medium|low"
}
```

### Variable Substitution:
- `{{email}}` - User email address
- `{{password}}` - User password  
- `{{api_key}}` - API key
- `{{senderName}}` - Extracted sender name
- `{{subject}}` - Extracted email subject

## 🚀 Getting Started Checklist

1. ✅ **Install dependencies**: `npm install`
2. ✅ **Set up API key**: Add to `.env` file
3. ✅ **Test browser visibility**: `npm run test-visibility`
4. ✅ **Try dynamic auth**: `npm run test-dynamic`
5. ✅ **Run your SOP**: `npm run test-your-sop`

## 🎉 Success Indicators

You'll know it's working when you see:
- ✅ Browser window opens and is clearly visible
- ✅ DevTools panel appears on the right side
- ✅ AI successfully navigates to websites
- ✅ Authentication prompts appear when needed
- ✅ Credentials are accepted and cached
- ✅ Workflow steps execute in sequence
- ✅ Data extraction works correctly

## 🔧 Troubleshooting

### Common Issues:

**Browser not visible:**
- Run `npm run test-visibility`
- Check all desktop spaces
- Ensure Chrome/Chromium is installed

**Authentication failures:**
- Use App Passwords for Gmail (not regular password)
- Check credential format and spelling
- Verify account access permissions

**AI model errors:**
- Verify API key is correct
- Check model name spelling
- Ensure sufficient API credits

**Step execution failures:**
- Check `stagehand_instruction` clarity
- Verify website structure hasn't changed
- Review error logs for specific issues

## 📈 Performance Tips

- **Use Gemini 2.5 Flash** for fastest execution
- **Enable visual mode** for debugging (`headless: false`)
- **Adjust slowMo** for observation vs speed
- **Cache credentials** to avoid repeated prompts
- **Use atomic steps** for better error recovery

---

**Ready to test AI agent execution of your workflows? Start with `npm run test-dynamic`!** 🚀

## 🎉 Success Criteria

- ✅ **80%+ success rate** on atomic steps
- ✅ **Reliable data extraction** between applications
- ✅ **Effective error recovery** mechanisms
- ✅ **Fast execution** (< 5 seconds per step average)

This validates that our **SOP → AI Agent** pipeline is production-ready! 🚀 

## 📁 File Structure

```
stagehand-test/
├── package.json              # Dependencies and scripts
├── stagehand.config.js       # Stagehand configuration
├── sop-executor.js          # Main SOP execution engine
├── test-gmail-steps.js      # Test atomic Gmail steps
├── test-full-sop.js         # Test complete SOP workflow
├── sample-sop.json          # Generated sample SOP
└── README.md               # This file
```

## 🔧 Core Components

### SOPExecutor Class
The main execution engine that:
- ✅ Executes atomic steps using `page.act()` and `page.extract()`
- ✅ Handles error recovery automatically
- ✅ Tracks execution metrics and success rates
- ✅ Manages variable substitution between steps
- ✅ Provides detailed logging and analysis

### Step Types Supported
1. **`task`** - Execute actions using `page.act()`
2. **`extract`** - Extract data using `page.extract()` with Zod schemas
3. **`decision`** - Make boolean decisions based on page state

## 🎯 What We're Testing

### Atomic Granularity
- Can Stagehand execute each individual step reliably?
- Are the instructions specific enough for AI agents?
- Do the error recovery mechanisms work?

### Variable Flow
- Does data extraction work correctly?
- Can variables be passed between steps?
- Are Zod schemas properly parsed?

### Success Metrics
- **Success Rate**: % of steps that execute successfully
- **Execution Time**: Average time per step
- **Error Recovery**: How often recovery mechanisms work

## 📊 Expected Results

### High Success Rate (80%+)
Indicates that our giga-optimized prompt generates SOPs with excellent atomic granularity suitable for AI agent execution.

### Moderate Success Rate (60-80%)
Some steps need refinement, but the approach is sound.

### Low Success Rate (<60%)
Prompt needs further optimization for atomic granularity.

## 🔍 Analysis Features

The test suite provides:
- ✅ **Step-by-step execution logs**
- ✅ **Success rate analysis**
- ✅ **Performance metrics**
- ✅ **Failed step identification**
- ✅ **Variable extraction tracking**
- ✅ **Error recovery monitoring**

## 🎭 Stagehand vs Browser-Use

This test environment helps validate whether **Stagehand** (TypeScript) or **Browser-Use** (Python) is better suited for executing our atomic SOPs.

### Stagehand Advantages
- 🎯 Built specifically for AI agent web automation
- 🔧 Excellent error handling and recovery
- 📊 Strong data extraction with Zod schemas
- 🚀 Optimized for reliability over speed

### Browser-Use Advantages  
- 🐍 Python ecosystem integration
- 🔄 More flexible for custom logic
- 📈 Better for complex decision trees

## 🚀 Next Steps

1. **Run Tests**: Execute the test suite with your OpenAI API key
2. **Analyze Results**: Review success rates and failed steps
3. **Iterate Prompt**: Refine the giga-optimized prompt based on failures
4. **Compare Agents**: Test the same SOPs with Browser-Use
5. **Production Integration**: Integrate the best-performing agent into AEF

## 🎉 Success Criteria

- ✅ **80%+ success rate** on atomic steps
- ✅ **Reliable data extraction** between applications
- ✅ **Effective error recovery** mechanisms
- ✅ **Fast execution** (< 5 seconds per step average)

This validates that our **SOP → AI Agent** pipeline is production-ready! 🚀 