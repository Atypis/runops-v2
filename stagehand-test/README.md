# ğŸ­ Stagehand Test Environment

This directory contains a complete testing environment for validating **Stagehand AI agent execution** of SOPs generated by our giga-optimized prompt.

## ğŸ¯ Purpose

Test the atomic granularity and execution reliability of SOPs generated by the **Stagehand-optimized prompt v0.91** compared to the original prompt.

## ğŸš€ Quick Start - Dynamic Authentication (Recommended)

The **dynamic authentication approach** is much more flexible and user-friendly:

```bash
# 1. Install dependencies
npm install

# 2. Test browser visibility first
npm run test-visibility

# 3. Run dynamic SOP execution (no upfront auth needed!)
npm run test-dynamic
```

### âœ¨ Benefits of Dynamic Auth:
- ğŸ­ **Browser opens immediately** - no waiting for credential setup
- ğŸ” **AI requests credentials only when needed** - more natural workflow  
- ğŸ¤– **Conversational approach** - AI "talks back" when it needs auth
- âš¡ **Faster startup** - no upfront configuration required
- ğŸ”„ **Session caching** - credentials remembered during execution

## ğŸ¯ Available Test Scripts

| Script | Purpose | Authentication |
|--------|---------|----------------|
| `npm run test-dynamic` | **Dynamic auth demo** (recommended) | On-demand prompts |
| `npm run test-visibility` | Enhanced browser visibility test | None |
| `npm run test-your-sop` | Your actual SOP workflow | Pre-configured |
| `npm run test-gmail` | Simple Gmail automation | Pre-configured |
| `npm run demo` | Visual demonstration | None |
| `npm run compare` | Multi-model comparison | Pre-configured |

## ğŸ”§ Configuration

### LLM Configuration
Edit `stagehand.config.js` to configure your preferred AI model:

```javascript
export default {
  llmConfig: {
    modelName: "google/gemini-2.5-flash-preview-05-20", // or gpt-4o, claude-3-5-sonnet
    modelClientOptions: {
      apiKey: process.env.GEMINI_API_KEY, // or OPENAI_API_KEY, ANTHROPIC_API_KEY
    }
  }
};
```

### Environment Variables
Create `.env` file:
```bash
# Choose your preferred LLM
GEMINI_API_KEY=your_gemini_key_here
OPENAI_API_KEY=your_openai_key_here  
ANTHROPIC_API_KEY=your_anthropic_key_here

# Optional: Pre-configure credentials (not needed for dynamic auth)
GMAIL_EMAIL=your_email@gmail.com
GMAIL_PASSWORD=your_app_password
AIRTABLE_EMAIL=your_email@example.com
AIRTABLE_PASSWORD=your_password
```

## ğŸ­ Dynamic Authentication Flow

The dynamic approach works like this:

1. **Browser opens immediately** - no credential prompts upfront
2. **AI navigates to login screens** - Gmail, Airtable, etc.
3. **AI detects authentication needed** - analyzes page content
4. **System prompts for credentials** - only when actually needed
5. **Credentials cached for session** - no repeated prompts
6. **AI continues workflow** - seamless execution

### Example Dynamic Flow:
```
ğŸš€ Browser opens â†’ Gmail.com
ğŸ¤– AI: "I see a login screen"
ğŸ” System: "Gmail Authentication Required - Enter email:"
ğŸ‘¤ User: enters email
ğŸ” System: "Enter password:" 
ğŸ‘¤ User: enters password (masked)
âœ… Credentials cached for session
ğŸ¤– AI: continues with inbox access
```

## ğŸ¬ Execution Examples

### Dynamic Authentication (Recommended)
```bash
npm run test-dynamic
```
- Browser opens immediately
- AI requests auth when needed
- Natural conversational flow
- Session credential caching

### Enhanced Browser Visibility
```bash
npm run test-visibility  
```
- Tests browser window visibility
- macOS optimized settings
- DevTools integration
- Window positioning and focus

### Your Actual SOP
```bash
npm run test-your-sop
```
- Executes your recorded workflow
- 15 atomic steps from investor CRM
- Gmail â†’ Airtable automation
- Pre-configured credentials

## ğŸ” Browser Visibility Troubleshooting

If you can't see the browser window:

1. **Check all desktop spaces** - browser might open in different space
2. **Look for Chrome/Chromium** - window might be behind other apps
3. **Check Dock** - browser icon should appear when running
4. **Run visibility test**:
   ```bash
   npm run test-visibility
   ```
5. **macOS permissions** - ensure automation permissions are granted

### Enhanced Visibility Features:
- `--start-maximized` - large window size
- `--auto-open-devtools-for-tabs` - DevTools visible
- `--window-position=100,100` - specific positioning
- `--activate-on-open` - brings window to front
- `slowMo: 1000` - slower actions for observation

## ğŸ“Š Model Comparison

Test different AI models:
```bash
npm run compare
```

Supported models:
- **Gemini 2.5 Flash Preview** (recommended for speed)
- **GPT-4o** (excellent reasoning)
- **Claude 3.5 Sonnet** (great for complex tasks)
- **Ollama local models** (privacy-focused)

## ğŸ” Security Features

### Dynamic Auth Security:
- **Memory-only storage** - credentials never written to disk
- **Session-scoped** - cleared when execution ends
- **Masked input** - passwords hidden during entry
- **No hardcoding** - credentials requested dynamically

### Pre-configured Auth Security:
- **Environment variables** - credentials in `.env` file
- **Interactive prompts** - secure password masking
- **Automatic cleanup** - memory cleared after execution

## ğŸ¯ SOP Structure

Your SOPs should include these fields for optimal execution:

```json
{
  "id": "step_id",
  "type": "task|extract|decision", 
  "label": "Human readable description",
  "stagehand_instruction": "AI instruction with {{variable}} placeholders",
  "extract_instruction": "What to extract from page",
  "extract_schema": "Zod schema string",
  "error_recovery": ["fallback instruction 1", "fallback instruction 2"],
  "confidence_level": "high|medium|low"
}
```

### Variable Substitution:
- `{{email}}` - User email address
- `{{password}}` - User password  
- `{{api_key}}` - API key
- `{{senderName}}` - Extracted sender name
- `{{subject}}` - Extracted email subject

## ğŸš€ Getting Started Checklist

1. âœ… **Install dependencies**: `npm install`
2. âœ… **Set up API key**: Add to `.env` file
3. âœ… **Test browser visibility**: `npm run test-visibility`
4. âœ… **Try dynamic auth**: `npm run test-dynamic`
5. âœ… **Run your SOP**: `npm run test-your-sop`

## ğŸ‰ Success Indicators

You'll know it's working when you see:
- âœ… Browser window opens and is clearly visible
- âœ… DevTools panel appears on the right side
- âœ… AI successfully navigates to websites
- âœ… Authentication prompts appear when needed
- âœ… Credentials are accepted and cached
- âœ… Workflow steps execute in sequence
- âœ… Data extraction works correctly

## ğŸ”§ Troubleshooting

### Common Issues:

**Browser not visible:**
- Run `npm run test-visibility`
- Check all desktop spaces
- Ensure Chrome/Chromium is installed

**Authentication failures:**
- Use App Passwords for Gmail (not regular password)
- Check credential format and spelling
- Verify account access permissions

**AI model errors:**
- Verify API key is correct
- Check model name spelling
- Ensure sufficient API credits

**Step execution failures:**
- Check `stagehand_instruction` clarity
- Verify website structure hasn't changed
- Review error logs for specific issues

## ğŸ“ˆ Performance Tips

- **Use Gemini 2.5 Flash** for fastest execution
- **Enable visual mode** for debugging (`headless: false`)
- **Adjust slowMo** for observation vs speed
- **Cache credentials** to avoid repeated prompts
- **Use atomic steps** for better error recovery

---

**Ready to test AI agent execution of your workflows? Start with `npm run test-dynamic`!** ğŸš€

## ğŸ‰ Success Criteria

- âœ… **80%+ success rate** on atomic steps
- âœ… **Reliable data extraction** between applications
- âœ… **Effective error recovery** mechanisms
- âœ… **Fast execution** (< 5 seconds per step average)

This validates that our **SOP â†’ AI Agent** pipeline is production-ready! ğŸš€ 

## ğŸ“ File Structure

```
stagehand-test/
â”œâ”€â”€ package.json              # Dependencies and scripts
â”œâ”€â”€ stagehand.config.js       # Stagehand configuration
â”œâ”€â”€ sop-executor.js          # Main SOP execution engine
â”œâ”€â”€ test-gmail-steps.js      # Test atomic Gmail steps
â”œâ”€â”€ test-full-sop.js         # Test complete SOP workflow
â”œâ”€â”€ sample-sop.json          # Generated sample SOP
â””â”€â”€ README.md               # This file
```

## ğŸ”§ Core Components

### SOPExecutor Class
The main execution engine that:
- âœ… Executes atomic steps using `page.act()` and `page.extract()`
- âœ… Handles error recovery automatically
- âœ… Tracks execution metrics and success rates
- âœ… Manages variable substitution between steps
- âœ… Provides detailed logging and analysis

### Step Types Supported
1. **`task`** - Execute actions using `page.act()`
2. **`extract`** - Extract data using `page.extract()` with Zod schemas
3. **`decision`** - Make boolean decisions based on page state

## ğŸ¯ What We're Testing

### Atomic Granularity
- Can Stagehand execute each individual step reliably?
- Are the instructions specific enough for AI agents?
- Do the error recovery mechanisms work?

### Variable Flow
- Does data extraction work correctly?
- Can variables be passed between steps?
- Are Zod schemas properly parsed?

### Success Metrics
- **Success Rate**: % of steps that execute successfully
- **Execution Time**: Average time per step
- **Error Recovery**: How often recovery mechanisms work

## ğŸ“Š Expected Results

### High Success Rate (80%+)
Indicates that our giga-optimized prompt generates SOPs with excellent atomic granularity suitable for AI agent execution.

### Moderate Success Rate (60-80%)
Some steps need refinement, but the approach is sound.

### Low Success Rate (<60%)
Prompt needs further optimization for atomic granularity.

## ğŸ” Analysis Features

The test suite provides:
- âœ… **Step-by-step execution logs**
- âœ… **Success rate analysis**
- âœ… **Performance metrics**
- âœ… **Failed step identification**
- âœ… **Variable extraction tracking**
- âœ… **Error recovery monitoring**

## ğŸ­ Stagehand vs Browser-Use

This test environment helps validate whether **Stagehand** (TypeScript) or **Browser-Use** (Python) is better suited for executing our atomic SOPs.

### Stagehand Advantages
- ğŸ¯ Built specifically for AI agent web automation
- ğŸ”§ Excellent error handling and recovery
- ğŸ“Š Strong data extraction with Zod schemas
- ğŸš€ Optimized for reliability over speed

### Browser-Use Advantages  
- ğŸ Python ecosystem integration
- ğŸ”„ More flexible for custom logic
- ğŸ“ˆ Better for complex decision trees

## ğŸš€ Next Steps

1. **Run Tests**: Execute the test suite with your OpenAI API key
2. **Analyze Results**: Review success rates and failed steps
3. **Iterate Prompt**: Refine the giga-optimized prompt based on failures
4. **Compare Agents**: Test the same SOPs with Browser-Use
5. **Production Integration**: Integrate the best-performing agent into AEF

## ğŸ‰ Success Criteria

- âœ… **80%+ success rate** on atomic steps
- âœ… **Reliable data extraction** between applications
- âœ… **Effective error recovery** mechanisms
- âœ… **Fast execution** (< 5 seconds per step average)

This validates that our **SOP â†’ AI Agent** pipeline is production-ready! ğŸš€ 