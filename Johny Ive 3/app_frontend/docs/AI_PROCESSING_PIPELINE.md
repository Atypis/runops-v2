# ü§ñ AI Processing Pipeline Guide

> **Comprehensive Documentation - Video ‚Üí Transcript ‚Üí SOP Transformation**

## üîç Pipeline Overview

### Two-Stage AI Processing Architecture
```
Stage 1: Video ‚Üí Detailed Transcript (Gemini 2.5 Flash Preview)
                     ‚Üì
Stage 2: Transcript ‚Üí Structured SOP (Gemini 2.5 Flash Preview)
```

**Rationale**: Two-stage approach provides better accuracy and allows for transcript inspection/debugging.

## üìπ Video Processing & Optimization (FFmpeg)

### Current Video Processing Settings (worker.js:202-229)
```javascript
// VERIFIED IMPLEMENTATION
ffmpeg(inputPath)
  .outputOptions([
    '-vf fps=1',        // 1 frame per second (massive size reduction)
    '-vf scale=-2:720', // 720p height, preserve aspect ratio  
    '-crf 32',          // Compression factor (higher = smaller file)
  ])
  .output(outputPath)
  .run();
```

### Processing Pipeline Stages
```
Raw Video (750MB max) ‚Üí FFmpeg Processing ‚Üí Slim Video (~5-50MB)
                                               ‚Üì
                               Base64 Encode ‚Üí Gemini API Input
```

### Video Size Optimization Strategy
- **Frame Rate**: Reduced to 1fps (from typical 24-60fps)
- **Resolution**: Maximum 720p height (common workflows don't need 4K)
- **Compression**: CRF 32 (good balance of quality vs size)
- **Format**: MP4 container (best compatibility with Gemini API)

### Size Reduction Results
- **Before**: 750MB raw video (20min @ 1080p)
- **After**: ~20-50MB slim video (1fps @ 720p)
- **Compression Ratio**: ~95% size reduction typical

## üéØ Stage 1: Video Transcription

### Prompt Engineering (prompts/transcription_prompt_v1.md)
```markdown
# VERIFIED PROMPT STRUCTURE

You are an expert at analyzing screen recordings to create detailed transcriptions.
Analyze this video and create a comprehensive transcript that captures:

1. Visual elements (UI components, screens, applications)
2. User actions (clicks, typing, navigation)  
3. Audio content (if present)
4. Timing and sequence information

Output as JSON array with this structure:
[
  {
    "timestamp": "approximate time",
    "visual": "what's visible on screen",
    "action": "what the user does",
    "audio": "any spoken content",
    "context": "additional relevant details"
  }
]
```

### Transcript Data Structure
```typescript
interface TranscriptEntry {
  timestamp: string;      // "0:30" or "2:15" format
  visual: string;         // Screen content description
  action: string;         // User action description  
  audio?: string;         // Spoken content (if any)
  context?: string;       // Additional context
}

type TranscriptData = TranscriptEntry[];
```

### Stage 1 Processing Logic (worker.js:300-373)
```javascript
async function transcribeVideo(videoPath) {
  const MAX_RETRIES = 3;
  let attempt = 0;
  let transcriptData = null;
  
  // Convert video to base64 for Gemini API
  const fileBuffer = fs.readFileSync(videoPath);
  const base64Video = fileBuffer.toString('base64');
  
  // Get Gemini model
  const model = genAI.getGenerativeModel({ 
    model: "gemini-2.5-flash-preview-04-17" 
  });
  
  while (attempt < MAX_RETRIES && !transcriptData) {
    attempt++;
    
    try {
      const result = await model.generateContent([
        TRANSCRIPTION_PROMPT,
        {
          inlineData: {
            mimeType: "video/mp4",
            data: base64Video
          }
        }
      ]);
      
      const response = await result.response;
      const text = response.text();
      
      // Clean up response (remove markdown code blocks)
      let cleanedText = text.replace(/```json\n|\n```|```/g, '');
      cleanedText = cleanedText.trim();
      
      // Parse as JSON array
      transcriptData = JSON.parse(cleanedText);
      
    } catch (error) {
      console.error(`Transcription attempt ${attempt} failed:`, error);
      
      if (attempt < MAX_RETRIES) {
        await new Promise(resolve => setTimeout(resolve, 5000));
      }
    }
  }
  
  return transcriptData;
}
```

### Transcription Quality Factors
- **Video Quality**: 720p provides sufficient detail for UI recognition
- **Frame Rate**: 1fps captures key workflow steps without redundancy
- **Model Selection**: Gemini 2.5 Flash Preview has strong video understanding
- **Prompt Specificity**: Structured output format ensures consistent parsing

## üèóÔ∏è Stage 2: SOP Generation

### Prompt Engineering (prompts/flat_sop_parser_v0.9 copy.md)
```markdown
# VERIFIED SOP PARSING PROMPT

You are an expert at analyzing video transcripts and creating structured SOPs.

Transform the provided transcript into a hierarchical SOP document.

Key Requirements:
1. Identify distinct workflow steps
2. Detect decision points (if/then logic)
3. Recognize loops or repeated actions
4. Create proper node relationships
5. Generate clear, actionable step descriptions

Output JSON structure:
{
  "meta": {
    "title": "Workflow Name",
    "description": "Brief description",
    "estimated_duration": "X minutes"
  },
  "public": {
    "nodes": [
      {
        "id": "unique_id",
        "type": "step|decision|loop|trigger|end",
        "label": "Human-readable title",
        "description": "Detailed instructions",
        "parentId": "parent_node_id",
        "children": ["child_node_ids"]
      }
    ],
    "edges": [
      {
        "id": "edge_id",
        "source": "source_node_id", 
        "target": "target_node_id",
        "condition": "yes|no|next"
      }
    ]
  }
}
```

### SOP Data Structure (lib/types/sop.ts)
```typescript
interface SOPNode {
  id: string;
  type: 'trigger' | 'step' | 'decision' | 'loop' | 'end';
  label: string;
  description?: string;
  parentId?: string;
  children?: string[];
  id_path?: string;        // For hierarchical numbering
}

interface SOPEdge {
  id: string;
  source: string;
  target: string;  
  condition?: string;      // 'yes', 'no', 'next', etc.
  decision_path?: boolean; // Decision edge flag
  animated?: boolean;
}

interface SOPDocument {
  meta: {
    title: string;
    description: string;
    estimated_duration?: string;
  };
  public: {
    nodes: SOPNode[];
    edges: SOPEdge[];
  };
}
```

### Stage 2 Processing Logic (worker.js:374-467)
```javascript
async function generateSopFromTranscript(transcript) {
  const MAX_RETRIES = 3;
  let attempt = 0;
  let sopData = null;
  
  const model = genAI.getGenerativeModel({ 
    model: "gemini-2.5-flash-preview-04-17" 
  });
  
  // Combine prompt with transcript data
  const transcriptJson = JSON.stringify(transcript);
  const combinedPrompt = `${SOP_PARSING_PROMPT}\n\nTranscript:\n${transcriptJson}`;
  
  while (attempt < MAX_RETRIES && !sopData) {
    attempt++;
    
    try {
      const result = await model.generateContent(combinedPrompt);
      const response = await result.response;
      const text = response.text();
      
      // Clean up response
      let cleanedText = text.replace(/```json\n|\n```|```/g, '');
      cleanedText = cleanedText.trim();
      
      // Extract JSON object (find first { to last })
      const firstBrace = cleanedText.indexOf('{');
      const lastBrace = cleanedText.lastIndexOf('}');
      
      if (firstBrace !== -1 && lastBrace !== -1) {
        cleanedText = cleanedText.substring(firstBrace, lastBrace + 1);
      }
      
      sopData = JSON.parse(cleanedText);
      
      // Fallback JSON extraction if parsing fails
      if (!sopData) {
        const jsonRegex = /{(?:[^{}]|{(?:[^{}]|{[^{}]*})*})*}/g;
        const matches = cleanedText.match(jsonRegex);
        if (matches?.length > 0) {
          const largestMatch = matches.reduce((a, b) => 
            a.length > b.length ? a : b
          );
          sopData = JSON.parse(largestMatch);
        }
      }
      
    } catch (error) {
      console.error(`SOP generation attempt ${attempt} failed:`, error);
      
      if (attempt < MAX_RETRIES) {
        await new Promise(resolve => setTimeout(resolve, 5000));
      }
    }
  }
  
  return sopData;
}
```

## üéõÔ∏è Model Configuration & Settings

### Gemini 2.5 Flash Preview Capabilities
- **Video Processing**: Native video understanding (up to ~20MB efficiently)
- **Context Window**: Large enough for full transcripts + prompts
- **Response Format**: JSON generation with good reliability
- **Speed**: Fast response times suitable for real-time processing

### Performance Characteristics
- **Transcription Time**: 30-60 seconds for typical workflow video
- **SOP Generation Time**: 15-30 seconds for transcript processing
- **Total Processing**: ~2-5 minutes end-to-end
- **Success Rate**: ~90% with retry logic (3 attempts per stage)

### Error Patterns & Recovery
```javascript
// Common issues and handling:

1. JSON Parsing Errors
   ‚Üí Fallback regex extraction
   ‚Üí Markdown code block cleanup
   ‚Üí Retry with fresh request

2. API Rate Limits  
   ‚Üí 5-second backoff between retries
   ‚Üí Maximum 3 attempts per stage
   ‚Üí Circuit breaker recommended for production

3. Context Window Limits
   ‚Üí Video compression keeps input size manageable
   ‚Üí Transcript chunking not yet needed

4. Model Availability
   ‚Üí Single model dependency risk
   ‚Üí Consider fallback to gemini-1.5-pro if needed
```

## üîÑ Data Transformation Pipeline

### Transcript ‚Üí SOP Transformation Process
```
Raw Transcript Array ‚Üí Prompt Engineering ‚Üí Gemini API ‚Üí Raw SOP JSON
                                                              ‚Üì
JSON Validation ‚Üí Node Relationship Processing ‚Üí Edge Generation ‚Üí Storage
                                                              ‚Üì  
Database Storage ‚Üí ReactFlow Transformation ‚Üí UI Visualization
```

### Key Transformation Logic (lib/sop-utils.ts)
```typescript
// Transform SOP document to ReactFlow data
export const transformSopToFlowData = (sopDocument: SOPDocument) => {
  const flowNodes: FlowNode[] = [];
  const flowEdges: FlowEdge[] = [];
  
  // Process nodes
  sopDocument.public.nodes.forEach(node => {
    // Determine ReactFlow node type
    let nodeType = 'step';
    if (node.type === 'trigger') nodeType = 'trigger';
    else if (node.type === 'decision') nodeType = 'decision';
    else if (node.type === 'loop') nodeType = 'loop';
    else if (node.type === 'end') nodeType = 'end';
    
    // Handle container nodes (loops with children)
    if (node.children?.length > 0) {
      const containerWidth = Math.ceil(Math.sqrt(node.children.length));
      node.calculatedWidth = Math.max(450, containerWidth * 250);
      node.calculatedHeight = Math.max(300, containerWidth * 150);
    }
    
    flowNodes.push({
      id: node.id,
      type: nodeType,
      data: {
        label: node.label,
        description: node.description,
        childSopNodeIds: node.children || [],
        calculatedWidth: node.calculatedWidth,
        calculatedHeight: node.calculatedHeight
      },
      position: { x: 0, y: 0 }, // Layout engine will position
      parentNode: node.parentId
    });
  });
  
  // Process edges with enhanced metadata
  sopDocument.public.edges.forEach(edge => {
    flowEdges.push({
      id: edge.id,
      source: edge.source,
      target: edge.target,
      label: edge.condition,
      type: 'custom-edge',
      data: {
        condition: !!edge.condition,
        fromDecision: edge.decision_path || false
      }
    });
  });
  
  return { flowNodes, flowEdges };
};
```

## üìä Quality Metrics & Optimization

### Transcription Quality Indicators
- **Completeness**: All workflow steps captured
- **Accuracy**: UI elements correctly identified  
- **Temporal Ordering**: Steps in correct sequence
- **Action Clarity**: User actions clearly described

### SOP Generation Quality Indicators
- **Node Types**: Appropriate classification (step vs decision vs loop)
- **Relationships**: Correct parent-child and edge connections
- **Labels**: Clear, actionable step descriptions
- **Structure**: Logical flow with proper entry/exit points

### Performance Monitoring
```javascript
// Add performance tracking (recommended enhancement)
const trackProcessingMetrics = {
  transcriptionTime: Date.now(),
  sopGenerationTime: null,
  totalTokensUsed: 0,
  retryCount: 0,
  
  logTranscriptionComplete() {
    this.sopGenerationTime = Date.now();
    const duration = this.sopGenerationTime - this.transcriptionTime;
    console.log(`Transcription completed in ${duration}ms`);
  },
  
  logSopComplete() {
    const totalDuration = Date.now() - this.transcriptionTime;
    console.log(`Total AI processing: ${totalDuration}ms`);
  }
};
```

## üéØ Optimization Strategies

### Current Optimizations
1. **Video Compression**: 95% size reduction via FFmpeg
2. **Two-Stage Processing**: Better error isolation and debugging
3. **Retry Logic**: 3 attempts per stage with backoff
4. **JSON Cleanup**: Multiple parsing strategies for robustness

### Recommended Enhancements
```javascript
// 1. Prompt Caching (reduce token usage)
const cachedPrompts = new Map();

// 2. Chunked Processing (for very long videos)
const chunkTranscript = (transcript, maxChunkSize = 50) => {
  const chunks = [];
  for (let i = 0; i < transcript.length; i += maxChunkSize) {
    chunks.push(transcript.slice(i, i + maxChunkSize));
  }
  return chunks;
};

// 3. Quality Scoring (validate output quality)
const scoreSOPQuality = (sopData) => {
  const metrics = {
    hasValidStructure: !!(sopData.meta && sopData.public),
    hasNodes: sopData.public?.nodes?.length > 0,
    hasEdges: sopData.public?.edges?.length > 0,
    hasStartNode: sopData.public?.nodes?.some(n => n.type === 'trigger'),
    hasEndNode: sopData.public?.nodes?.some(n => n.type === 'end')
  };
  
  return Object.values(metrics).filter(Boolean).length / Object.keys(metrics).length;
};
```

## üîß Debugging & Troubleshooting

### Common Issues & Solutions

#### 1. Video Too Large for API
```javascript
// Current limit: ~20MB after compression
// Solution: Increase compression or chunk video
ffmpeg(inputPath)
  .outputOptions(['-crf 35']) // Higher compression
  .run();
```

#### 2. Inconsistent JSON Output
```javascript
// Enhanced JSON extraction
const extractValidJSON = (text) => {
  const cleaningStrategies = [
    (t) => t.replace(/```json\n|\n```|```/g, ''),
    (t) => t.substring(t.indexOf('{'), t.lastIndexOf('}') + 1),
    (t) => t.match(/{.*}/s)?.[0] || t
  ];
  
  for (const strategy of cleaningStrategies) {
    try {
      return JSON.parse(strategy(text));
    } catch (e) {
      continue;
    }
  }
  throw new Error('No valid JSON found');
};
```

#### 3. Poor SOP Quality
```markdown
Symptoms: Missing steps, incorrect relationships, unclear labels

Solutions:
1. Improve video quality (ensure clear UI visibility)
2. Enhance prompts with better examples
3. Add post-processing validation
4. Implement feedback loops for iterative improvement
```

### Testing AI Pipeline
```bash
# Test transcription only
node -e "
const { transcribeVideo } = require('./worker.js');
transcribeVideo('./test-video.mp4').then(console.log);
"

# Test SOP generation only  
node -e "
const { generateSopFromTranscript } = require('./worker.js');
const transcript = require('./test-transcript.json');
generateSopFromTranscript(transcript).then(console.log);
"

# Full pipeline test
npm run test-worker
```

---

## üéØ Quick Reference

### File Locations
- **Transcription Prompt**: `prompts/transcription_prompt_v1.md`
- **SOP Parsing Prompt**: `prompts/flat_sop_parser_v0.9 copy.md`
- **Pipeline Logic**: `worker.js` lines 300-467
- **Data Transformation**: `lib/sop-utils.ts`
- **Type Definitions**: `lib/types/sop.ts`

### Key Constants
- **Max Video Size**: 750MB input, ~20MB after compression
- **Processing Time**: 2-5 minutes typical
- **Retry Attempts**: 3 per stage
- **Backoff Delay**: 5 seconds between retries

---

## üîó Related Documentation
- **Upload Process**: See `UploadProcessingArchitecture.md` for complete upload ‚Üí processing flow
- **Worker Operations**: See `WorkerInstructions.md` for operational details and troubleshooting  
- **Performance**: See `PERFORMANCE_OPTIMIZATION_GUIDE.md` for AI pipeline optimization
- **Error Handling**: See `ERROR_HANDLING_GUIDE.md` for debugging AI processing issues

**Last Updated**: Dec 2024 | **Status**: Production-ready pipeline documentation 