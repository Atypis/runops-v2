# âš¡ Performance Optimization Guide

> **AI Agent Performance Manual - System Optimization & Scalability Strategies**

## ðŸ“Š Current Performance Baseline

### Processing Time Benchmarks (Dec 2024)
```
Video Upload:      2-30 seconds (depending on file size)
Video Compression: 30-90 seconds (720p, 1fps, CRF 32)
AI Transcription:  30-60 seconds (Gemini 2.5 Flash Preview)
SOP Generation:    15-30 seconds (Gemini text processing)
Total End-to-End:  2-5 minutes typical workflow
```

### Resource Consumption
```
Memory Usage:      200-500MB peak (during video processing)
Storage:          95% reduction (750MB â†’ 20-50MB compressed)
API Calls:        2-6 Gemini requests per SOP (with retries)
Database Queries: ~10-15 per complete workflow
```

### System Limits (Current)
```
Max Video Size:    750MB input file
Max Duration:      20 minutes video length
Max SOP Nodes:     ~50 nodes (ReactFlow performance limit)
Concurrent Jobs:   Limited by Gemini API quota
```

## ðŸŽ¥ Video Processing Optimization

### FFmpeg Configuration Optimization (worker.js)
```javascript
// CURRENT OPTIMIZED SETTINGS (VERIFIED)
const optimizeVideo = (inputPath, outputPath) => {
  return new Promise((resolve, reject) => {
    ffmpeg(inputPath)
      .outputOptions([
        '-vf fps=1',                    // 1fps: Minimal frames for workflow capture
        '-vf scale=-2:720',             // 720p height: Good quality/size balance
        '-crf 32',                      // High compression: Acceptable quality loss
        '-preset fast',                 // Speed over compression efficiency
        '-movflags +faststart',         // Web optimization: Progressive download
        '-tune stillimage',             // Optimize for low motion content
        '-pix_fmt yuv420p'              // Broad compatibility
      ])
      .videoCodec('libx264')            // H.264: Best compatibility
      .audioCodec('aac')                // AAC: Standard audio codec
      .audioBitrate('64k')              // Low audio bitrate
      .on('end', resolve)
      .on('error', reject)
      .save(outputPath);
  });
};
```

### Enhanced Video Processing Strategy
```javascript
// RECOMMENDED IMPROVEMENTS
const intelligentVideoOptimization = async (inputPath, outputPath) => {
  // 1. Video analysis first
  const metadata = await getVideoMetadata(inputPath);
  const { duration, bitrate, resolution } = metadata;
  
  // 2. Adaptive compression based on content
  let crf = 32;  // Default
  if (duration > 600) crf = 35;      // Longer videos: more compression
  if (bitrate > 10000) crf = 30;     // High quality source: less compression
  
  // 3. Dynamic frame rate optimization
  const fps = duration > 300 ? 0.5 : 1;  // Ultra-long videos: 0.5fps
  
  // 4. Progressive encoding for large files (near max limit)
  if (metadata.size > 400 * 1024 * 1024) {  // 400MB+ (near 750MB limit)
    return await progressiveEncode(inputPath, outputPath, { crf, fps });
  }
  
  // Standard encoding
  return await standardEncode(inputPath, outputPath, { crf, fps });
};

const progressiveEncode = async (input, output, options) => {
  // Encode in chunks for memory efficiency
  const tempDir = path.join(__dirname, 'temp_chunks');
  await fs.ensureDir(tempDir);
  
  try {
    // Split into manageable chunks
    const chunks = await splitVideo(input, 300); // 5-minute chunks
    const processedChunks = [];
    
    for (const chunk of chunks) {
      const processed = await encodeChunk(chunk, options);
      processedChunks.push(processed);
    }
    
    // Concatenate processed chunks
    await concatenateChunks(processedChunks, output);
    
  } finally {
    await fs.remove(tempDir);
  }
};
```

### Video Preprocessing Optimizations
```javascript
// Size estimation before processing
const estimateOutputSize = (inputSize, duration) => {
  // Empirical formula based on current settings
  const reductionRatio = 0.95; // 95% reduction typical
  const estimatedSize = inputSize * (1 - reductionRatio);
  
  // Adjust for duration (longer videos compress better)
  const durationFactor = Math.min(duration / 600, 2); // Max 2x improvement
  return estimatedSize / durationFactor;
};

// Early abort for oversized content
const validateVideoForProcessing = async (filePath) => {
  const metadata = await getVideoMetadata(filePath);
  
  if (metadata.duration > 1800) {  // 30+ minutes
    throw new Error('Video too long for efficient processing');
  }
  
  if (metadata.bitrate > 50000) {  // Very high bitrate
    console.warn('High bitrate detected, processing may be slow');
  }
  
  return metadata;
};
```

## ðŸ¤– AI Processing Pipeline Optimization

### Gemini API Optimization Strategy
```javascript
// CURRENT RETRY LOGIC (VERIFIED - worker.js:300-467)
const optimizedGeminiCall = async (prompt, videoData, maxRetries = 3) => {
  let attempt = 0;
  const backoffDelays = [1000, 3000, 5000]; // Progressive backoff
  
  while (attempt < maxRetries) {
    try {
      // Add request caching for identical video content
      const cacheKey = createCacheKey(videoData);
      const cachedResult = await getFromCache(cacheKey);
      
      if (cachedResult) {
        console.log('Cache hit for video processing');
        return cachedResult;
      }
      
      const result = await model.generateContent([prompt, videoData]);
      const response = await result.response;
      
      // Cache successful results
      await setCache(cacheKey, response, 3600); // 1 hour cache
      
      return response;
      
    } catch (error) {
      attempt++;
      
      if (error.status === 429) {  // Rate limit
        const delay = backoffDelays[Math.min(attempt - 1, 2)] * 2; // Double for rate limits
        console.log(`Rate limited, waiting ${delay}ms...`);
        await sleep(delay);
      } else if (attempt < maxRetries) {
        await sleep(backoffDelays[Math.min(attempt - 1, 2)]);
      } else {
        throw error;
      }
    }
  }
};

// Request caching implementation
const createCacheKey = (videoData) => {
  const videoHash = crypto
    .createHash('md5')
    .update(videoData)
    .digest('hex')
    .substring(0, 16);
  return `video_${videoHash}`;
};
```

### Prompt Optimization for Speed
```javascript
// ENHANCED PROMPTS FOR FASTER PROCESSING
const OPTIMIZED_TRANSCRIPTION_PROMPT = `
Analyze this workflow video efficiently. Focus on:
- Key UI changes (skip redundant frames)
- User actions only (ignore passive displays)
- Critical decision points
- Essential timing information

Output as concise JSON array, maximum 20 entries:
[{"timestamp": "0:30", "action": "clicked login", "screen": "login page"}]
`;

const OPTIMIZED_SOP_PROMPT = `
Convert transcript to SOP. Generate MAXIMUM 15 nodes total.
Focus on:
- Essential workflow steps only
- Clear decision points (yes/no)
- No redundant intermediate steps
- Logical flow optimization

Output minimal JSON structure:
{"meta": {"title": "..."}, "public": {"nodes": [...], "edges": [...]}}
`;
```

### Parallel Processing Strategy
```javascript
// RECOMMENDED: Parallel processing for multiple stages
const parallelProcessingPipeline = async (jobId) => {
  try {
    // Stage 1: Video processing (blocking)
    await updateJobStatus(jobId, 'processing', null, 'preparing_video', 10);
    const { rawPath, slimPath } = await processVideo(jobId);
    
    // Stage 2 & 3: Parallel AI processing
    await updateJobStatus(jobId, 'processing', null, 'ai_processing', 40);
    
    const [transcript, videoAnalysis] = await Promise.all([
      transcribeVideo(slimPath),
      analyzeVideoMetadata(rawPath)  // Extract metadata in parallel
    ]);
    
    // Stage 4: SOP generation with enhanced context
    await updateJobStatus(jobId, 'processing', null, 'parsing_sop', 70);
    const sopData = await generateSopFromTranscript(transcript, videoAnalysis);
    
    // Stage 5: Finalization
    await updateJobStatus(jobId, 'processing', null, 'finalizing', 90);
    await saveSopToDatabase(jobId, sopData);
    
    await updateJobStatus(jobId, 'completed');
    
  } catch (error) {
    console.error(`Parallel processing failed for job ${jobId}:`, error);
    await updateJobStatus(jobId, 'error', error.message);
  }
};
```

## ðŸ’¾ Database Query Optimization

### Current Query Patterns (Verified)
```sql
-- Job status polling (most frequent query)
SELECT * FROM jobs WHERE job_id = $1;

-- SOP retrieval with RLS
SELECT * FROM sops WHERE job_id = $1 AND user_id = auth.uid();

-- Worker job polling
SELECT * FROM jobs WHERE status = 'queued' ORDER BY created_at ASC LIMIT 1;
```

### Database Index Optimization
```sql
-- RECOMMENDED INDEXES FOR PERFORMANCE

-- Primary job polling index
CREATE INDEX CONCURRENTLY idx_jobs_status_created 
ON jobs(status, created_at) 
WHERE status IN ('queued', 'processing');

-- Job ID lookup optimization (already exists as primary key)
CREATE UNIQUE INDEX idx_jobs_job_id ON jobs(job_id);

-- SOP access optimization (RLS compatible)
CREATE INDEX idx_sops_job_id_user_id ON sops(job_id, user_id);

-- Recent jobs for user dashboard
CREATE INDEX idx_jobs_user_created ON jobs(user_id, created_at DESC)
WHERE status = 'completed';

-- Status monitoring index
CREATE INDEX idx_jobs_status_updated ON jobs(status, updated_at)
WHERE status IN ('processing', 'error');
```

### Query Optimization Strategies
```typescript
// Efficient job polling with timeout
const pollJobsEfficiently = async () => {
  // 1. Use LIMIT 1 with ORDER BY for queue processing
  const { data: jobs } = await supabase
    .from('jobs')
    .select('*')
    .eq('status', 'queued')
    .order('created_at', { ascending: true })
    .limit(1);
  
  if (jobs.length === 0) {
    // 2. Exponential backoff when queue is empty
    const emptyQueueDelay = Math.min(currentDelay * 1.5, 30000);
    await sleep(emptyQueueDelay);
    return;
  }
  
  // 3. Process immediately when jobs available
  await processJob(jobs[0]);
};

// Optimized status update with minimal data
const updateJobStatusOptimized = async (jobId, status, error = null, stage = null, percent = null) => {
  const updates = {
    status,
    updated_at: new Date().toISOString()
  };
  
  // Only include non-null optional fields
  if (error) updates.error = error;
  if (stage) updates.progress_stage = stage;
  if (percent !== null) updates.progress_percent = percent;
  if (status === 'completed') updates.completed_at = new Date().toISOString();
  
  const { error: updateError } = await supabase
    .from('jobs')
    .update(updates)
    .eq('job_id', jobId);
    
  if (updateError) {
    console.error('Failed to update job status:', updateError);
  }
};
```

### Connection Pool Optimization
```typescript
// Database connection management
const createOptimizedSupabaseClient = () => {
  return createClient(
    process.env.NEXT_PUBLIC_SUPABASE_URL,
    process.env.SUPABASE_SERVICE_ROLE_KEY,
    {
      db: {
        schema: 'public'
      },
      auth: {
        autoRefreshToken: false,
        persistSession: false
      },
      global: {
        headers: {
          'Connection': 'keep-alive'
        }
      }
    }
  );
};

// Connection reuse for worker
let workerSupabaseClient = null;
const getWorkerDatabase = () => {
  if (!workerSupabaseClient) {
    workerSupabaseClient = createOptimizedSupabaseClient();
  }
  return workerSupabaseClient;
};
```

## ðŸ–¥ï¸ Frontend Performance Optimization

### ReactFlow Rendering Optimization
```typescript
// CURRENT OPTIMIZATIONS (VERIFIED - SOPFlowView.tsx)
const memoizedNodeTypes = useMemo(() => ({
  trigger: TriggerNode,
  step: StepNode,
  end: EndNode,
  decision: DecisionNode,
  loop: LoopNode,
}), []);

// ENHANCED OPTIMIZATIONS (RECOMMENDED)
const OptimizedSOPFlowView = React.memo(({ sopData }) => {
  // 1. Lazy load large SOPs
  const isLargeSOP = sopData.public.nodes.length > 30;
  
  if (isLargeSOP) {
    return <VirtualizedSOPView sopData={sopData} />;
  }
  
  // 2. Debounced layout calculations
  const debouncedLayout = useMemo(
    () => debounce(getLayoutedElements, 250),
    []
  );
  
  // 3. Virtualized node rendering for containers
  const renderNode = useCallback((node) => {
    if (node.type === 'loop' && node.data?.childSopNodeIds?.length > 10) {
      return <VirtualizedLoopNode node={node} />;
    }
    return <StandardNode node={node} />;
  }, []);
  
  // 4. Edge optimization for complex flows
  const optimizedEdges = useMemo(() => {
    if (edges.length > 50) {
      return simplifyEdgeRouting(edges);
    }
    return edges;
  }, [edges]);
  
  return (
    <ReactFlow
      nodes={nodes}
      edges={optimizedEdges}
      nodeTypes={memoizedNodeTypes}
      onNodesChange={debouncedOnNodesChange}
      // Performance settings
      fitView
      maxZoom={1.5}
      minZoom={0.3}
      defaultViewport={{ x: 0, y: 0, zoom: 0.6 }}
    />
  );
});
```

### Upload Progress Optimization
```typescript
// Optimized upload with progress tracking
const optimizedUpload = async (file, onProgress) => {
  const CHUNK_SIZE = 5 * 1024 * 1024; // 5MB chunks
  
  if (file.size > CHUNK_SIZE * 2) {
    return await chunkedUpload(file, onProgress);
  }
  
  // Direct upload for smaller files
  return await directUpload(file, onProgress);
};

const chunkedUpload = async (file, onProgress) => {
  const totalChunks = Math.ceil(file.size / CHUNK_SIZE);
  let uploadedBytes = 0;
  
  for (let i = 0; i < totalChunks; i++) {
    const start = i * CHUNK_SIZE;
    const end = Math.min(start + CHUNK_SIZE, file.size);
    const chunk = file.slice(start, end);
    
    await uploadChunk(chunk, i);
    
    uploadedBytes += chunk.size;
    onProgress((uploadedBytes / file.size) * 100);
  }
};
```

### Status Polling Optimization
```typescript
// Intelligent polling with backoff
class StatusPollingOptimizer {
  constructor(jobId) {
    this.jobId = jobId;
    this.pollCount = 0;
    this.baseDelay = 2000;
    this.maxDelay = 30000;
  }
  
  async poll() {
    const response = await fetch(`/api/job-status/${this.jobId}`);
    const status = await response.json();
    
    if (status.status === 'completed' || status.status === 'error') {
      return status;
    }
    
    // Adaptive polling frequency
    const delay = this.calculateDelay(status);
    await this.sleep(delay);
    
    return this.poll();
  }
  
  calculateDelay(status) {
    this.pollCount++;
    
    // Faster polling during active processing
    if (status.progress_percent > 0) {
      return Math.min(this.baseDelay * 1.2 ** this.pollCount, 10000);
    }
    
    // Slower polling when queued
    return Math.min(this.baseDelay * 1.5 ** this.pollCount, this.maxDelay);
  }
}
```

## ðŸ”„ System Architecture Optimization

### Worker Process Optimization
```javascript
// ENHANCED WORKER ARCHITECTURE
class OptimizedWorker {
  constructor() {
    this.activeJobs = new Map();
    this.maxConcurrentJobs = 3;  // Process multiple jobs
    this.healthCheckInterval = 30000;
    this.lastHealthCheck = Date.now();
  }
  
  async start() {
    // Health monitoring
    setInterval(() => this.healthCheck(), this.healthCheckInterval);
    
    // Main processing loop
    while (true) {
      try {
        if (this.activeJobs.size < this.maxConcurrentJobs) {
          const job = await this.getNextJob();
          if (job) {
            this.processJobConcurrently(job);
          } else {
            await this.sleep(5000); // No jobs available
          }
        } else {
          await this.sleep(2000); // At capacity
        }
      } catch (error) {
        console.error('Worker loop error:', error);
        await this.sleep(10000); // Recover from errors
      }
    }
  }
  
  async processJobConcurrently(job) {
    const jobId = job.job_id;
    this.activeJobs.set(jobId, { startTime: Date.now(), job });
    
    try {
      await this.processJob(job);
    } finally {
      this.activeJobs.delete(jobId);
    }
  }
  
  async healthCheck() {
    const now = Date.now();
    
    // Check for stuck jobs
    for (const [jobId, jobInfo] of this.activeJobs) {
      const processingTime = now - jobInfo.startTime;
      if (processingTime > 600000) { // 10 minutes
        console.warn(`Job ${jobId} stuck for ${processingTime}ms`);
        await this.handleStuckJob(jobId, jobInfo);
      }
    }
    
    // System resource check
    const memUsage = process.memoryUsage();
    if (memUsage.heapUsed > 1000000000) { // 1GB
      console.warn('High memory usage detected:', memUsage);
      global.gc?.(); // Force garbage collection if available
    }
    
    this.lastHealthCheck = now;
  }
}
```

### Caching Strategy
```javascript
// Multi-layer caching system
class CacheManager {
  constructor() {
    this.memoryCache = new Map();
    this.diskCache = new Map();
    this.maxMemoryItems = 100;
  }
  
  async get(key) {
    // 1. Memory cache (fastest)
    if (this.memoryCache.has(key)) {
      const item = this.memoryCache.get(key);
      if (item.expires > Date.now()) {
        return item.value;
      }
      this.memoryCache.delete(key);
    }
    
    // 2. Disk cache (slower but persistent)
    const diskItem = await this.getDiskCache(key);
    if (diskItem) {
      // Promote to memory cache
      this.setMemoryCache(key, diskItem.value, diskItem.expires);
      return diskItem.value;
    }
    
    return null;
  }
  
  async set(key, value, ttlMs = 3600000) { // 1 hour default
    const expires = Date.now() + ttlMs;
    
    // Set in both caches
    this.setMemoryCache(key, value, expires);
    await this.setDiskCache(key, value, expires);
  }
  
  setMemoryCache(key, value, expires) {
    // Evict oldest if at capacity
    if (this.memoryCache.size >= this.maxMemoryItems) {
      const firstKey = this.memoryCache.keys().next().value;
      this.memoryCache.delete(firstKey);
    }
    
    this.memoryCache.set(key, { value, expires });
  }
}

// Usage in worker
const cacheManager = new CacheManager();

const transcribeVideoWithCache = async (videoPath) => {
  const videoHash = await getFileHash(videoPath);
  const cacheKey = `transcript_${videoHash}`;
  
  let transcript = await cacheManager.get(cacheKey);
  if (transcript) {
    console.log('Using cached transcript');
    return transcript;
  }
  
  transcript = await transcribeVideo(videoPath);
  await cacheManager.set(cacheKey, transcript, 86400000); // 24 hours
  
  return transcript;
};
```

## ðŸ“ˆ Monitoring & Metrics

### Performance Metrics Collection
```javascript
// Performance monitoring system
class PerformanceMonitor {
  constructor() {
    this.metrics = {
      processedJobs: 0,
      totalProcessingTime: 0,
      errorCount: 0,
      cacheHitRate: 0,
      memoryUsage: []
    };
  }
  
  recordJobProcessing(jobId, startTime, endTime, success) {
    const processingTime = endTime - startTime;
    
    this.metrics.processedJobs++;
    this.metrics.totalProcessingTime += processingTime;
    
    if (!success) {
      this.metrics.errorCount++;
    }
    
    // Log detailed metrics
    console.log(`Job ${jobId} processed in ${processingTime}ms (success: ${success})`);
    
    // Store in database for analysis
    this.storeMetrics(jobId, {
      processing_time: processingTime,
      success,
      timestamp: new Date().toISOString()
    });
  }
  
  getAverageProcessingTime() {
    return this.metrics.processedJobs > 0 
      ? this.metrics.totalProcessingTime / this.metrics.processedJobs 
      : 0;
  }
  
  getErrorRate() {
    return this.metrics.processedJobs > 0 
      ? this.metrics.errorCount / this.metrics.processedJobs 
      : 0;
  }
}
```

### System Health Dashboard (Recommended)
```typescript
// api/health/detailed/route.ts
export async function GET() {
  const metrics = await gatherSystemMetrics();
  
  return NextResponse.json({
    status: determineSystemHealth(metrics),
    metrics: {
      activeJobs: metrics.activeJobs,
      queueLength: metrics.queueLength,
      averageProcessingTime: metrics.avgProcessingTime,
      errorRate: metrics.errorRate,
      systemResources: {
        memoryUsage: process.memoryUsage(),
        uptime: process.uptime()
      },
      dependencies: {
        database: await testDatabaseHealth(),
        storage: await testStorageHealth(),
        geminiApi: await testGeminiHealth()
      }
    },
    recommendations: generateRecommendations(metrics)
  });
}
```

## ðŸš€ Scalability Recommendations

### Horizontal Scaling Strategy
```javascript
// Multi-worker architecture
const WORKER_TYPES = {
  VIDEO_PROCESSOR: 'video_processor',    // FFmpeg operations
  AI_PROCESSOR: 'ai_processor',          // Gemini API calls
  GENERAL: 'general'                     // Mixed workload
};

class ScalableWorkerManager {
  constructor() {
    this.workers = new Map();
    this.jobQueue = new PriorityQueue();
    this.loadBalancer = new LoadBalancer();
  }
  
  async distributeJob(job) {
    const jobType = this.determineJobType(job);
    const optimalWorker = this.loadBalancer.selectWorker(jobType);
    
    return await this.assignJobToWorker(job, optimalWorker);
  }
  
  determineJobType(job) {
    // Route based on job requirements
    if (job.stage === 'preparing_video') return WORKER_TYPES.VIDEO_PROCESSOR;
    if (job.stage === 'transcribing' || job.stage === 'parsing_sop') return WORKER_TYPES.AI_PROCESSOR;
    return WORKER_TYPES.GENERAL;
  }
}
```

### Performance Optimization Summary

#### Quick Wins (Immediate Implementation)
1. **Add database indexes** for job polling and SOP retrieval
2. **Implement request caching** for Gemini API calls
3. **Enable FFmpeg fast preset** and progressive encoding
4. **Add status polling backoff** to reduce API load
5. **Implement memory cleanup** in worker process

#### Medium-term Improvements (1-2 weeks)
1. **Parallel AI processing** for transcription + metadata analysis
2. **Chunked video upload** for large files
3. **ReactFlow virtualization** for complex SOPs
4. **Worker health monitoring** with automatic restart
5. **Enhanced error recovery** with exponential backoff

#### Long-term Scalability (1+ months)
1. **Multi-worker architecture** with job type routing
2. **Distributed caching** with Redis or similar
3. **Video preprocessing pipeline** with quality detection
4. **Advanced AI prompt optimization** with A/B testing
5. **Comprehensive monitoring dashboard** with alerting

---

## ðŸŽ¯ Quick Performance Checklist

### Before Deploying New Features
- [ ] Add appropriate database indexes
- [ ] Implement caching where beneficial
- [ ] Test with large video files (400MB+ approaching 750MB limit)
- [ ] Verify memory usage patterns
- [ ] Check error handling under load

### Monitoring Alert Thresholds
- [ ] Processing time > 10 minutes per job
- [ ] Error rate > 5%
- [ ] Memory usage > 1GB sustained
- [ ] Queue length > 10 jobs
- [ ] Database query time > 1 second

### Performance Testing Commands
```bash
# Load test upload endpoint
curl -X GET http://localhost:3000/api/get-upload-url

# Monitor worker memory
ps aux | grep worker.js

# Database query performance
psql -c "EXPLAIN ANALYZE SELECT * FROM jobs WHERE status = 'queued';"

# Check file system cache
du -sh app_frontend/temp/
```

**Last Updated**: Dec 2024 | **Status**: Comprehensive optimization guide with actionable recommendations 